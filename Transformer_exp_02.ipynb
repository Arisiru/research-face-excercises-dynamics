{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ffceac",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fce85",
   "metadata": {},
   "source": [
    "Second iteration is to build own input from the raw csv\n",
    "- every patient is a dictionary\n",
    "- every exercise in an entry\n",
    "- every exercise has meta data\n",
    "- every entry consist of timeserieses for all face points \n",
    "- a training/test split shall be based on patients so we could see a progression of exercises\n",
    "\n",
    "\n",
    "use https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ to mix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2167fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2277c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa1148",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- move to the environment file all global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb75db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 2050\n",
    "X_MAX = 362.850308418274\n",
    "X_MIN = -385.386139154434\n",
    "Y_MAX = 407.209008932114\n",
    "Y_MIN = -184.533506631851\n",
    "Z_MAX = 1414.35980796814\n",
    "Z_MIN = 465.504199266434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d69a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {\n",
    "    '0_LefteyeMidbottom': 'orbital', \n",
    "    '1_LefteyeMidtop': 'orbital',  \n",
    "    '2_LefteyeInnercorner': 'orbital', \n",
    "    '3_LefteyeOutercorner': 'orbital',  \n",
    "    '4_LefteyebrowInner': 'frontal', \n",
    "    '5_LefteyebrowCenter': 'frontal',  \n",
    "    '6_RighteyeMidbottom': 'orbital',  \n",
    "    '7_RighteyeMidtop': 'orbital', \n",
    "    '8_RighteyeInnercorner': 'orbital',  \n",
    "    '9_RighteyeOutercorner': 'orbital', \n",
    "    '10_RighteyebrowInner': 'frontal', \n",
    "    '11_RighteyebrowCenter': 'frontal',  \n",
    "    '12_NoseTip': 'nasal', \n",
    "    '13_MouthLowerlipMidbottom': 'oral',\n",
    "    '14_MouthLeftcorner': 'oral',\n",
    "    '15_MouthRightcorner': 'oral',\n",
    "    '16_MouthUpperlipMidtop': 'oral',\n",
    "    '17_ChinCenter': 'other', \n",
    "    '18_ForeheadCenter': 'frontal', \n",
    "    '19_LeftcheekCenter': 'other', \n",
    "    '20_RightcheekCenter': 'other',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_root = os.path.join('data')\n",
    "dir_data_source = os.path.join(dir_data_root, 'csv')\n",
    "dir_data_target = os.path.join(dir_data_root, 'json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544c090",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba082d6",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- read data from folder\n",
    "- for every file read it to form sequences for every  poi\n",
    "- create a list of sequences per poi as exercise\n",
    "- create a list of metadata for an exercise\n",
    "- add an exercise to correct patient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de383d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(xs, ys, zs):\n",
    "    distance = [0]\n",
    "    for i in range(1, len(xs)):\n",
    "        distance.append(math.dist([xs[i-1], ys[i-1], zs[i-1]], [xs[i], ys[i], zs[i]]))\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a1377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(value, min, max):\n",
    "    return (value - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fefbcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    return vector / np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc6953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_two_points(p1, p2):\n",
    "    p1_u = unit_vector(p1)\n",
    "    p2_u = unit_vector(p2)\n",
    "    return np.arccos(np.clip(np.dot(p1_u, p2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc16888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_angles(xs, ys, zs):\n",
    "    angles = [0]\n",
    "    for i in range(1, len(xs)):\n",
    "        angles.append(angle_between_two_points([xs[i-1], ys[i-1], zs[i-1]], [xs[i], ys[i], zs[i]]))\n",
    "    \n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f02512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_metadata(file_name):\n",
    "    meta = file_name.split(' ')\n",
    "    patient_id = meta[0]\n",
    "    try:\n",
    "        exercise_dates = datetime.strptime(re.sub(r'\\_[0-9]', '', meta[1]), '%Y-%m-%d') \n",
    "    except:\n",
    "        print(file_name)\n",
    "        exercise_dates = ''\n",
    "        \n",
    "    evaluation = int(meta[2].replace('eval', ''))\n",
    "    flag_before_surgery = int(meta[3].replace('bf', '').replace('.csv', ''))\n",
    "    return {\n",
    "        'patient_id': patient_id,\n",
    "        'exercise_dates': exercise_dates.strftime('%Y-%m-%d'),\n",
    "        'evaluation': evaluation,\n",
    "        'flag_before_surgery': flag_before_surgery,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b85966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_excercise(directory, filename):   \n",
    "    file_meta = filename_to_metadata(file_name)\n",
    "    \n",
    "    patient_id = file_meta['patient_id']\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(directory, filename))\n",
    "    df.drop(['patient', 'date', 'before surgery', 'evaluation'], axis = 1, inplace = True)\n",
    "    \n",
    "    #error handlin in original data, to catch which file has error uncomment this\n",
    "    #errorrs_in_exercise_ = df[[not isinstance(value, str) for value in df['exercise']]]\n",
    "    #if len(errorrs_in_exercise_):\n",
    "    #    print(filename)\n",
    "    #    print(errorrs_in_exercise_)\n",
    "    \n",
    "    exercises = sorted(df['exercise'].astype(str).unique())\n",
    "\n",
    "    pois = sorted(df['point id'].unique())\n",
    "    session = {\n",
    "        'meta': file_meta,\n",
    "        'exercises': []\n",
    "    }\n",
    "    \n",
    "    for exercise in exercises:\n",
    "        df_exercise = df[(df['exercise'] == exercise)]\n",
    "        \n",
    "        exercise_data = {\n",
    "            'meta': {\n",
    "                'tag': exercise,\n",
    "                'id': int(exercise.split('_')[0]),\n",
    "                'name': exercise.split('_')[1]\n",
    "            },\n",
    "            'pois': [],\n",
    "        }\n",
    "\n",
    "        for poi in pois: \n",
    "            df_poi = df_exercise[(df_exercise['point id']) == poi]\n",
    "            df_poi = df_poi.sort_values(by=['t'])\n",
    "            df_poi = df_poi.drop(columns = ['t', 'exercise', 'point id'], axis=1)\n",
    "            \n",
    "            xs = df_poi['x'].tolist()\n",
    "            ys = df_poi['y'].tolist()\n",
    "            zs = df_poi['z'].tolist()\n",
    "            xs_normalized = [rescale(x, X_MIN, X_MAX) for x in xs]\n",
    "            ys_normalized = [rescale(y, Y_MIN, Y_MAX) for y in ys]\n",
    "            zs_normalized = [rescale(z, Z_MIN, Z_MAX) for z in zs]\n",
    "            \n",
    "            poi_data = {\n",
    "                'meta': {\n",
    "                    'tag': poi,\n",
    "                    'id': int(poi.split('_')[0]),\n",
    "                    'name': poi.split('_')[1],  \n",
    "                    'region': region[poi]\n",
    "                },\n",
    "                'input': {\n",
    "                    'x': xs,\n",
    "                    'y': ys,\n",
    "                    'z': zs,\n",
    "                    'dist': distance(xs, ys, zs),\n",
    "                    'x_normalized': xs_normalized,\n",
    "                    'y_normalized': ys_normalized,\n",
    "                    'z_normalized': zs_normalized,\n",
    "                    'dist_normalized': distance(xs_normalized, ys_normalized, zs_normalized),\n",
    "                    'direction_angles': direction_angles(xs, ys, zs)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            exercise_data['pois'].append(poi_data)\n",
    "            \n",
    "        session['exercises'].append(exercise_data)\n",
    "        \n",
    "    return patient_id, session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab6ece",
   "metadata": {},
   "source": [
    "Read every file in the target directory and apply mapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256cd9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000078\n",
      "00000000000\n",
      "00000000055\n",
      "00000000041\n",
      "00000000015\n",
      "00000000067\n",
      "00000000070\n",
      "00000000016\n",
      "00000000065\n",
      "00000000019\n",
      "00000000003\n",
      "00000000007\n",
      "00000000038\n",
      "00000000014\n",
      "00000000047\n",
      "00000000039\n",
      "00000000002\n",
      "00000000042\n",
      "00000000036\n",
      "00000000023\n",
      "00000000035\n",
      "00000000068\n",
      "00000000010\n",
      "00000000011\n",
      "00000000040\n",
      "00000000029\n",
      "00000000046\n",
      "00000000045\n",
      "00000000028\n",
      "00000000072\n",
      "00000000076\n",
      "00000000026\n",
      "00000000052\n",
      "00000000018\n",
      "00000000057\n",
      "00000000001\n",
      "00000000017\n",
      "00000000025\n",
      "00000000059\n",
      "00000000080\n",
      "00000000081\n",
      "00000000048\n",
      "00000000064\n",
      "00000000033\n",
      "00000000006\n",
      "00000000082\n",
      "00000000083\n",
      "00000000063\n",
      "00000000009\n",
      "00000000069\n",
      "00000000060\n",
      "00000000012\n",
      "00000000073\n",
      "00000000053\n",
      "00000000066\n",
      "00000000027\n",
      "00000000020\n",
      "00000000074\n",
      "00000000051\n",
      "00000000030\n",
      "00000000031\n",
      "00000000085\n",
      "00000000061\n",
      "00000000071\n",
      "00000000056\n",
      "00000000004\n",
      "00000000005\n",
      "00000000050\n",
      "00000000079\n",
      "00000000049\n",
      "00000000034\n",
      "00000000044\n",
      "00000000008\n",
      "00000000043\n",
      "00000000013\n",
      "00000000077\n",
      "00000000022\n",
      "00000000037\n",
      "00000000058\n",
      "00000000075\n",
      "00000000032\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "\n",
    "for file_name in os.listdir(dir_data_source):\n",
    "    \n",
    "    patient_id, session = file_to_excercise(dir_data_source, file_name) \n",
    "    \n",
    "    if patient_id not in data:\n",
    "        data[patient_id] = []\n",
    "    data[patient_id].append(session)    \n",
    "\n",
    "\n",
    "for patient_id in data:\n",
    "    print(patient_id)\n",
    "    with open(os.path.join(dir_data_target, '%s.json' % patient_id), \"w\") as f_w:\n",
    "        json.dump(data[patient_id], f_w, indent = 2)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda87fc2",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783fd9e",
   "metadata": {},
   "source": [
    "## Data set per session"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91aa8d7d",
   "metadata": {},
   "source": [
    "session = {\n",
    "    'meta': {\n",
    "        'patient_id': str,\n",
    "        'exercise_dates': str,\n",
    "        'evaluation': int,\n",
    "        'flag_before_surgery': flag_before_surgery,\n",
    "    }\n",
    "    'exercises': [...]\n",
    "}\n",
    "\n",
    "exercise_data = {\n",
    "    'meta': {\n",
    "        'tag': str\n",
    "        'id': int\n",
    "        'name': str\n",
    "    },\n",
    "    'pois': [...]\n",
    "}\n",
    "poi_data = {\n",
    "    'meta': {\n",
    "        'tag': str\n",
    "        'id': int\n",
    "        'name': str\n",
    "        'region': str\n",
    "    },\n",
    "    'input': {\n",
    "        'x': []\n",
    "        'y': []\n",
    "        'z': []\n",
    "        'dist': []\n",
    "        'x_normalized': []\n",
    "        'y_normalized': []\n",
    "        'z_normalized': []\n",
    "        'dist_normalized': []\n",
    "        'direction_angles': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3d18d",
   "metadata": {},
   "source": [
    "### Build training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93a7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sessions for patient: 00000000078\n",
      "Extracting sessions for patient: 00000000000\n",
      "Extracting sessions for patient: 00000000055\n",
      "Extracting sessions for patient: 00000000041\n",
      "Extracting sessions for patient: 00000000015\n",
      "Extracting sessions for patient: 00000000067\n",
      "Extracting sessions for patient: 00000000070\n",
      "Extracting sessions for patient: 00000000016\n",
      "Extracting sessions for patient: 00000000065\n",
      "Extracting sessions for patient: 00000000019\n",
      "Extracting sessions for patient: 00000000003\n",
      "Extracting sessions for patient: 00000000007\n",
      "Extracting sessions for patient: 00000000038\n",
      "Extracting sessions for patient: 00000000014\n",
      "Extracting sessions for patient: 00000000047\n",
      "Extracting sessions for patient: 00000000039\n",
      "Extracting sessions for patient: 00000000002\n",
      "Extracting sessions for patient: 00000000042\n",
      "Extracting sessions for patient: 00000000036\n",
      "Extracting sessions for patient: 00000000023\n",
      "Extracting sessions for patient: 00000000035\n",
      "Extracting sessions for patient: 00000000068\n",
      "Extracting sessions for patient: 00000000010\n",
      "Extracting sessions for patient: 00000000011\n",
      "Extracting sessions for patient: 00000000040\n",
      "Extracting sessions for patient: 00000000029\n",
      "Extracting sessions for patient: 00000000046\n",
      "Extracting sessions for patient: 00000000045\n",
      "Extracting sessions for patient: 00000000028\n",
      "Extracting sessions for patient: 00000000072\n",
      "Extracting sessions for patient: 00000000076\n",
      "Extracting sessions for patient: 00000000026\n",
      "Extracting sessions for patient: 00000000052\n",
      "Extracting sessions for patient: 00000000018\n",
      "Extracting sessions for patient: 00000000057\n",
      "Extracting sessions for patient: 00000000001\n",
      "Extracting sessions for patient: 00000000017\n",
      "Extracting sessions for patient: 00000000025\n",
      "Extracting sessions for patient: 00000000059\n",
      "Extracting sessions for patient: 00000000080\n",
      "Extracting sessions for patient: 00000000081\n",
      "Extracting sessions for patient: 00000000048\n",
      "Extracting sessions for patient: 00000000064\n",
      "Extracting sessions for patient: 00000000033\n",
      "Extracting sessions for patient: 00000000006\n",
      "Extracting sessions for patient: 00000000082\n",
      "Extracting sessions for patient: 00000000083\n",
      "Extracting sessions for patient: 00000000063\n",
      "Extracting sessions for patient: 00000000009\n",
      "Extracting sessions for patient: 00000000069\n",
      "Extracting sessions for patient: 00000000060\n",
      "Extracting sessions for patient: 00000000012\n",
      "Extracting sessions for patient: 00000000073\n",
      "Extracting sessions for patient: 00000000053\n",
      "Extracting sessions for patient: 00000000066\n",
      "Extracting sessions for patient: 00000000027\n",
      "Extracting sessions for patient: 00000000020\n",
      "Extracting sessions for patient: 00000000074\n",
      "Extracting sessions for patient: 00000000051\n",
      "Extracting sessions for patient: 00000000030\n",
      "Extracting sessions for patient: 00000000031\n",
      "Extracting sessions for patient: 00000000085\n",
      "Extracting sessions for patient: 00000000061\n",
      "Extracting sessions for patient: 00000000071\n",
      "Extracting sessions for patient: 00000000056\n",
      "Extracting sessions for patient: 00000000004\n",
      "Extracting sessions for patient: 00000000005\n",
      "Extracting sessions for patient: 00000000050\n",
      "Extracting sessions for patient: 00000000079\n",
      "Extracting sessions for patient: 00000000049\n",
      "Extracting sessions for patient: 00000000034\n",
      "Extracting sessions for patient: 00000000044\n",
      "Extracting sessions for patient: 00000000008\n",
      "Extracting sessions for patient: 00000000043\n",
      "Extracting sessions for patient: 00000000013\n",
      "Extracting sessions for patient: 00000000077\n",
      "Extracting sessions for patient: 00000000022\n",
      "Extracting sessions for patient: 00000000037\n",
      "Extracting sessions for patient: 00000000058\n",
      "Extracting sessions for patient: 00000000075\n",
      "Extracting sessions for patient: 00000000032\n"
     ]
    }
   ],
   "source": [
    "xslist = list()\n",
    "yslist = list()\n",
    "\n",
    "for patient_id in data:\n",
    "    print('Extracting sessions for patient: %s' % patient_id)\n",
    "    \n",
    "    for session in data[patient_id]:\n",
    "        for excercise in session['exercises']: \n",
    "            xs_excercise = []\n",
    "            for poi in excercise['pois']:\n",
    "                #xs_excercise.append(poi['input']['x'])\n",
    "                #xs_excercise.append(poi['input']['y'])\n",
    "                #xs_excercise.append(poi['input']['z'])\n",
    "                #xs_excercise.append(poi['input']['dist'])\n",
    "                xs_excercise.append(poi['input']['x_normalized'])\n",
    "                xs_excercise.append(poi['input']['y_normalized'])\n",
    "                xs_excercise.append(poi['input']['z_normalized'])\n",
    "                xs_excercise.append(poi['input']['dist_normalized'])\n",
    "                xs_excercise.append(poi['input']['direction_angles'])\n",
    "    \n",
    "            xslist.append(pad_sequences(\n",
    "                xs_excercise,\n",
    "                padding=\"pre\",\n",
    "                maxlen=SEQ_LEN))\n",
    "            yslist.append(session['meta']['evaluation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a14c7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xslist)\n",
    "len(yslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "580af428",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.array(yslist[:900])\n",
    "xs = np.array(xslist[:900])\n",
    "ys_test = np.array(yslist[900:])\n",
    "xs_test = np.array(xslist[900:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e43454e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900,)\n",
      "(900, 105, 2050)\n",
      "(129,)\n",
      "(129, 105, 2050)\n"
     ]
    }
   ],
   "source": [
    "print(ys.shape)\n",
    "print(xs.shape)\n",
    "print(ys_test.shape)\n",
    "print(xs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db63f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b33215bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeba3312",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b2c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (105, 2050)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 105, 2050)]       0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                266624    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 267,254\n",
      "Trainable params: 267,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputShape = xs[0].shape\n",
    "\n",
    "print('Input shape: %s' % (str(inputShape)))\n",
    "\n",
    "xs_input = Input(shape=inputShape)\n",
    "x = LSTM(32, activation=\"relu\")(xs_input)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dense(6, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=xs_input, outputs=x)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    optimizer=Adam(lr=1e-3, decay=1e-3 / 200),\n",
    "    metrics=['accuracy', f1_m, precision_m, recall_m]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ebf4216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 20:56:04.590151: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - ETA: 0s - loss: 1.3610 - accuracy: 0.4578 - f1_m: 2.9248 - precision_m: 111681360.0000 - recall_m: 1.5232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 21:10:27.118507: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 871s 8s/step - loss: 1.3610 - accuracy: 0.4578 - f1_m: 2.9248 - precision_m: 111681360.0000 - recall_m: 1.5232 - val_loss: 1.0360 - val_accuracy: 0.5814 - val_f1_m: 2.5294 - val_precision_m: 84705880.0000 - val_recall_m: 1.2647\n",
      "Epoch 2/5\n",
      "113/113 [==============================] - 862s 8s/step - loss: 1.1663 - accuracy: 0.4611 - f1_m: 2.1139 - precision_m: 69380520.0000 - recall_m: 1.2467 - val_loss: 0.9555 - val_accuracy: 0.5814 - val_f1_m: 1.6324 - val_precision_m: 1.6324 - val_recall_m: 1.6324\n",
      "Epoch 3/5\n",
      "113/113 [==============================] - 859s 8s/step - loss: 1.1440 - accuracy: 0.4611 - f1_m: 2.1803 - precision_m: 74867256.0000 - recall_m: 1.2445 - val_loss: 0.9527 - val_accuracy: 0.5814 - val_f1_m: 1.6324 - val_precision_m: 1.6324 - val_recall_m: 1.6324\n",
      "Epoch 4/5\n",
      "113/113 [==============================] - 859s 8s/step - loss: 1.1343 - accuracy: 0.4611 - f1_m: 2.0642 - precision_m: 49911500.0000 - recall_m: 1.4347 - val_loss: 0.9806 - val_accuracy: 0.5814 - val_f1_m: 2.4265 - val_precision_m: 84705880.0000 - val_recall_m: 1.2132\n",
      "Epoch 5/5\n",
      "113/113 [==============================] - 857s 8s/step - loss: 1.1441 - accuracy: 0.4611 - f1_m: 2.0985 - precision_m: 44955756.0000 - recall_m: 1.5299 - val_loss: 0.9906 - val_accuracy: 0.5814 - val_f1_m: 2.4265 - val_precision_m: 84705880.0000 - val_recall_m: 1.2132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173304220>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=xs, y=ys, \n",
    "    validation_data=(xs_test, ys_test),\n",
    "    batch_size=8, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8e3db",
   "metadata": {},
   "source": [
    "__END__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-prognosis",
   "language": "python",
   "name": "face-prognosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
