{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ffceac",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fce85",
   "metadata": {},
   "source": [
    "Data processing from \"raw\" json\n",
    "\n",
    "data to be split by excersise\n",
    "\n",
    "all pois shall be\n",
    "- without rebase\n",
    "    - normalised globaly\n",
    "    - normalised regionaly\n",
    "    - distances between time steps\n",
    "    - directional movment between time steps\n",
    "- with rebase\n",
    "    - normalised globaly with rebase on middle base (nose)\n",
    "    - normalised regionlay with regional rebase\n",
    "    - distances between time steps\n",
    "    - directional movment between time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedd1089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2277c",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed45b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_root = os.path.join('data')\n",
    "dir_data_source = os.path.join(dir_data_root, 'json')\n",
    "dir_data_target = os.path.join(dir_data_root, 'json_preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e544c090",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca024a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_order = {\n",
    "    '0_LefteyeMidbottom': 0,\n",
    "    '1_LefteyeMidtop': 1,\n",
    "    '2_LefteyeInnercorner': 2,\n",
    "    '3_LefteyeOutercorner': 3,  \n",
    "    '6_RighteyeMidbottom': 4,  \n",
    "    '7_RighteyeMidtop': 5,\n",
    "    '8_RighteyeInnercorner': 6,  \n",
    "    '9_RighteyeOutercorner': 7,\n",
    "    '4_LefteyebrowInner': 8,\n",
    "    '5_LefteyebrowCenter': 8,\n",
    "    '18_ForeheadCenter': 10,\n",
    "    '10_RighteyebrowInner': 11,\n",
    "    '11_RighteyebrowCenter': 12,\n",
    "    '12_NoseTip': 13,\n",
    "    '13_MouthLowerlipMidbottom': 14,\n",
    "    '14_MouthLeftcorner': 15,\n",
    "    '15_MouthRightcorner': 16,\n",
    "    '16_MouthUpperlipMidtop': 17,\n",
    "    '17_ChinCenter': 18,\n",
    "    '19_LeftcheekCenter': 19,\n",
    "    '20_RightcheekCenter': 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a1377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(value, minimum, maximum):\n",
    "    return (value - minimum) / (maximum - minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c050b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(xs):\n",
    "    xs_minimum = min(xs)\n",
    "    xs_maximum = max(xs)\n",
    "    \n",
    "    return [rescale(x, xs_minimum, xs_maximum) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de383d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(xs, ys, zs):\n",
    "    distance = [0]\n",
    "    for i in range(1, len(xs)):\n",
    "        distance.append(math.dist([xs[i-1], ys[i-1], zs[i-1]], [xs[i], ys[i], zs[i]]))\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "444134d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebase(xs, base_xs):\n",
    "    return[x - bx for x,bx in zip(xs, base_xs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefbcfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    return vector / np.linalg.norm(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc6953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_two_points(p1, p2):\n",
    "    p1_u = unit_vector(p1)\n",
    "    p2_u = unit_vector(p2)\n",
    "    return np.arccos(np.clip(np.dot(p1_u, p2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc16888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_angles(xs, ys, zs):\n",
    "    angles = [0]\n",
    "    for i in range(1, len(xs)):\n",
    "        angles.append(angle_between_two_points([xs[i-1], ys[i-1], zs[i-1]], [xs[i], ys[i], zs[i]]))\n",
    "    \n",
    "    return angles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01acc447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_pois(poi):\n",
    "    return pois_order[poi['meta']['tag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab6ece",
   "metadata": {},
   "source": [
    "Read every file in the target directory and apply mapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "237768e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data for patiend: 00000000082\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000057\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000000\n",
      "Processed 002 excersises\n",
      "Loading raw data for patiend: 00000000016\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000041\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000036\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000061\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000077\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000020\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000076\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000060\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000037\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000040\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000017\n",
      "Processed 036 excersises\n",
      "Loading raw data for patiend: 00000000001\n",
      "Processed 027 excersises\n",
      "Loading raw data for patiend: 00000000056\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000083\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000067\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000026\n",
      "Processed 027 excersises\n",
      "Loading raw data for patiend: 00000000051\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000006\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000010\n",
      "Processed 027 excersises\n",
      "Loading raw data for patiend: 00000000047\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000046\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000011\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000007\n",
      "Processed 027 excersises\n",
      "Loading raw data for patiend: 00000000050\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000085\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000027\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000070\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000066\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000031\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000049\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000008\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000073\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000065\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000032\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000045\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000012\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000004\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000028\n",
      "Processed 036 excersises\n",
      "Loading raw data for patiend: 00000000069\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000068\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000029\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000052\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000005\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000013\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000044\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000033\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000064\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000072\n",
      "Processed 013 excersises\n",
      "Loading raw data for patiend: 00000000025\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000009\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000048\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000043\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000014\n",
      "Processed 025 excersises\n",
      "Loading raw data for patiend: 00000000002\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000055\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000079\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000080\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000038\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000018\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000059\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000022\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000075\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000063\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000034\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000035\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000074\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000023\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000058\n",
      "Processed 006 excersises\n",
      "Loading raw data for patiend: 00000000019\n",
      "Processed 018 excersises\n",
      "Loading raw data for patiend: 00000000039\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000081\n",
      "Processed 009 excersises\n",
      "Loading raw data for patiend: 00000000078\n",
      "Processed 019 excersises\n",
      "Loading raw data for patiend: 00000000003\n",
      "Processed 027 excersises\n",
      "Loading raw data for patiend: 00000000015\n",
      "Processed 028 excersises\n",
      "Loading raw data for patiend: 00000000042\n",
      "Processed 009 excersises\n"
     ]
    }
   ],
   "source": [
    "amount_excercises = 0\n",
    "\n",
    "for file_name in os.listdir(dir_data_source):\n",
    "    with open(os.path.join(dir_data_source, file_name)) as f:\n",
    "       \n",
    "        patient_id = file_name.split('.')[0]\n",
    "        excercise_number = 0\n",
    "        print('Loading raw data for patiend: %s' % patient_id)\n",
    "        \n",
    "        data = json.load(f)\n",
    "        \n",
    "        for session in data:\n",
    "            for excercise in session['exercises']:\n",
    "                \n",
    "                excercise_number += 1\n",
    "                \n",
    "                excercise_meta = {\n",
    "                    'patient_id': session['meta']['patient_id'],\n",
    "                    'exercise_dates': session['meta']['exercise_dates'],\n",
    "                    'evaluation': session['meta']['evaluation'],\n",
    "                    'flag_before_surgery': session['meta']['flag_before_surgery'],\n",
    "                    'excercise_tag': excercise['meta']['tag'],\n",
    "                    'excercise_id': excercise['meta']['id'],\n",
    "                    'excercise_name': excercise['meta']['name']\n",
    "                }\n",
    "                \n",
    "                processed_excercise = {\n",
    "                    'meta': excercise_meta,\n",
    "                    'regions': {}\n",
    "                }\n",
    "                \n",
    "                base = {}\n",
    "                for poi in excercise['pois']:\n",
    "                    if poi['meta']['base'] == 'base':\n",
    "                        base = {\n",
    "                            'xs': poi['input']['xs'],\n",
    "                            'ys': poi['input']['ys'],\n",
    "                            'zs': poi['input']['zs'],\n",
    "                        }\n",
    "                        break\n",
    "                        \n",
    "                global_region = []\n",
    "                orbital_region = []\n",
    "                frontal_region = []\n",
    "                oral_region = []\n",
    "                global_region_rebased = []\n",
    "                orbital_region_rebased = []\n",
    "                frontal_region_rebased = []\n",
    "                oral_region_rebased = []\n",
    "                \n",
    "                pois_sorted = sorted(excercise['pois'], key = key_pois)\n",
    "                for poi in pois_sorted:\n",
    "                    xs = normalise(poi['input']['xs'])\n",
    "                    ys = normalise(poi['input']['ys'])\n",
    "                    zs = normalise(poi['input']['zs'])\n",
    "                    distances = distance(xs, ys, zs)\n",
    "                    #directions = direction_angles(xs, ys, zs)\n",
    "                    \n",
    "                    global_region.append(xs)\n",
    "                    global_region.append(ys)\n",
    "                    global_region.append(zs)\n",
    "                    global_region.append(distances)\n",
    "                    #global_region.append(directions)\n",
    "                    \n",
    "                    if poi['meta']['region'] == 'orbital':\n",
    "                        orbital_region.append(xs)\n",
    "                        orbital_region.append(ys)\n",
    "                        orbital_region.append(zs)\n",
    "                        orbital_region.append(distances)\n",
    "                        #orbital_region.append(directions)\n",
    "                    if poi['meta']['region'] == 'frontal':\n",
    "                        frontal_region.append(xs)\n",
    "                        frontal_region.append(ys)\n",
    "                        frontal_region.append(zs)\n",
    "                        frontal_region.append(distances)\n",
    "                        #frontal_region.append(directions) \n",
    "                    if poi['meta']['region'] == 'oral':\n",
    "                        oral_region.append(xs)\n",
    "                        oral_region.append(ys)\n",
    "                        oral_region.append(zs)\n",
    "                        oral_region.append(distances)\n",
    "                        #oral_region.append(directions)    \n",
    "                    \n",
    "                    processed_excercise['regions']['global'] = global_region\n",
    "                    processed_excercise['regions']['orbital'] = orbital_region\n",
    "                    processed_excercise['regions']['frontal'] = frontal_region\n",
    "                    processed_excercise['regions']['oral'] = oral_region\n",
    "                    \n",
    "                    #TODO start here        \n",
    "                    #TODO add rebase and normalise\n",
    "                    if poi['meta']['base'] == 'base':\n",
    "                        continue\n",
    "                                            \n",
    "                    xs_rebased = normalise(rebase(poi['input']['xs'], base['xs']))\n",
    "                    ys_rebased = normalise(rebase(poi['input']['ys'], base['ys']))\n",
    "                    zs_rebased = normalise(rebase(poi['input']['zs'], base['zs']))\n",
    "                    distances_rebased = distance(xs_rebased, ys_rebased, zs_rebased)\n",
    "                    \n",
    "                    global_region_rebased.append(xs_rebased)\n",
    "                    global_region_rebased.append(ys_rebased)\n",
    "                    global_region_rebased.append(zs_rebased)\n",
    "                    global_region_rebased.append(distances_rebased)\n",
    "                    \n",
    "                    if poi['meta']['region'] == 'orbital':\n",
    "                        orbital_region_rebased.append(xs_rebased)\n",
    "                        orbital_region_rebased.append(ys_rebased)\n",
    "                        orbital_region_rebased.append(zs_rebased)\n",
    "                        orbital_region_rebased.append(distances_rebased)\n",
    "                        #orbital_region.append(directions)\n",
    "                    if poi['meta']['region'] == 'frontal':\n",
    "                        frontal_region_rebased.append(xs_rebased)\n",
    "                        frontal_region_rebased.append(ys_rebased)\n",
    "                        frontal_region_rebased.append(zs_rebased)\n",
    "                        frontal_region_rebased.append(distances_rebased)\n",
    "                        #frontal_region.append(directions) \n",
    "                    if poi['meta']['region'] == 'oral':\n",
    "                        oral_region_rebased.append(xs_rebased)\n",
    "                        oral_region_rebased.append(ys_rebased)\n",
    "                        oral_region_rebased.append(zs_rebased)\n",
    "                        oral_region_rebased.append(distances_rebased)\n",
    "                        #oral_region.append(directions)  \n",
    "                \n",
    "                processed_excercise['regions']['global_rebased'] = global_region_rebased\n",
    "                processed_excercise['regions']['orbital_rebased'] = orbital_region_rebased\n",
    "                processed_excercise['regions']['frontal_rebased'] = frontal_region_rebased\n",
    "                processed_excercise['regions']['oral_rebased'] = oral_region_rebased\n",
    "                \n",
    "                with open(os.path.join(dir_data_target, '%s_%03d.json' % (patient_id, excercise_number)), \"w\") as f_w:\n",
    "                    json.dump(processed_excercise, f_w)\n",
    "                \n",
    "    print('Processed %03d excersises' % excercise_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783fd9e",
   "metadata": {},
   "source": [
    "## Data set per session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f82a54",
   "metadata": {},
   "source": [
    "## Input shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91aa8d7d",
   "metadata": {},
   "source": [
    "session = {\n",
    "    'meta': {\n",
    "        'patient_id': str,\n",
    "        'exercise_dates': str,\n",
    "        'evaluation': int,\n",
    "        'flag_before_surgery': flag_before_surgery,\n",
    "    }\n",
    "    'exercises': [...]\n",
    "}\n",
    "\n",
    "exercise_data = {\n",
    "    'meta': {\n",
    "        'tag': str\n",
    "        'id': int\n",
    "        'name': str\n",
    "    },\n",
    "    'pois': [...]\n",
    "}\n",
    "poi_data = {\n",
    "    'meta': {\n",
    "        'tag': str\n",
    "        'id': int\n",
    "        'name': str\n",
    "        'region': str\n",
    "        'base': str (tag of POI to rebase against)\n",
    "    },\n",
    "    'input': {\n",
    "        'ts': []\n",
    "        'xs': []\n",
    "        'ys': []\n",
    "        'zs': []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68949aa5",
   "metadata": {},
   "source": [
    "## Output shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "206b5c30",
   "metadata": {},
   "source": [
    "patien file is a list of excercises of objects\n",
    "\n",
    "    'meta': {\n",
    "        'patient_id': str,\n",
    "        'exercise_dates': str,\n",
    "        'evaluation': int,\n",
    "        'flag_before_surgery': int,\n",
    "        'excercise_tag': str\n",
    "        'excercise_id': int\n",
    "        'excercise_name': str\n",
    "    },\n",
    "    'regions': {\n",
    "        'global': [[float]] list of lists with normalised coordinates of all pois xs,ys,zs\n",
    "        'orbital': [[float]]\n",
    "        'frontal': [[float]]\n",
    "        'oral': [[float]]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde8e3db",
   "metadata": {},
   "source": [
    "__END__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face-prognosis",
   "language": "python",
   "name": "face-prognosis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
